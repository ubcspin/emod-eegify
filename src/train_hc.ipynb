{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669fe9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch\n",
    "# !pip3 install skorch\n",
    "# !pip3 install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66cb7bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p_all']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pylab import plot, show, savefig, xlim, figure, ylim, legend, boxplot, setp, axes\n",
    "\n",
    "import sys\n",
    "stdout = sys.stdout\n",
    "sys.stdout = stdout\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import make_scorer, confusion_matrix, fbeta_score\n",
    "\n",
    "# from training_touchtales.estimator_helper_hc import EstimatorSelectionHelper\n",
    "\n",
    "# from training_touchtales.models import MODELS, PARAMS\n",
    "\n",
    "from config_touchtale import TIME_INDEX, TIME_INTERVAL, WINDOW_SIZE, EXP_PARAMS, FS, N_FEATURES, SUBJECT_IDS\n",
    "\n",
    "from hiclass2 import metrics\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBClassifier\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "MODELS = {\n",
    "    # 'CNN': NeuralNetClassifier(module=cnn_classifier_original,\n",
    "    #     module__dropout=DROPOUT,\n",
    "    #     module__n_labels=LABEL_CLASS_COUNT,\n",
    "    #     optimizer=OPTIMIZER,\n",
    "    #     lr=LR,\n",
    "    #     max_epochs=MAX_EPOCHS,\n",
    "    #     criterion=CRITERION,\n",
    "    #     batch_size=BATCH_SIZE,\n",
    "    #     iterator_train__shuffle=True,\n",
    "    #     train_split=None,\n",
    "    #     device=DEVICE,\n",
    "    #     verbose=0)\n",
    "    'ExtraTreesClassifier': ExtraTreesClassifier(),\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'AdaBoostClassifier': AdaBoostClassifier(),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(),\n",
    "    # 'XGBClassifier': XGBClassifier(),\n",
    "    # 'SVC': SVC()  # takes a long time to train with linear kernels\n",
    "}\n",
    "\n",
    "PARAMS = {\n",
    "\n",
    "    # 'CNN': {\n",
    "    # 'local_classifier__batch_size': [128, 256, 512],\n",
    "    # 'local_classifier__lr': [LR*0.1, LR,  LR*10],\n",
    "    # 'local_classifier__max_epochs' : [MAX_EPOCHS,  MAX_EPOCHS*2, MAX_EPOCHS*4]\n",
    "    # }\n",
    "    'ExtraTreesClassifier': {'n_estimators': [N_FEATURES, 2*N_FEATURES]},\n",
    "    'RandomForestClassifier': {'n_estimators': [N_FEATURES, 2*N_FEATURES]},\n",
    "    'AdaBoostClassifier':  {'n_estimators': [N_FEATURES, 2*N_FEATURES]},\n",
    "    'GradientBoostingClassifier': {'n_estimators': [N_FEATURES, 2*N_FEATURES], 'learning_rate': [0.8, 1.0]},\n",
    "    # 'XGBClassifier': {'max_depth': (4, 6, 8), 'min_child_weight': (1, 5, 10)},\n",
    "    # 'SVC': [\n",
    "    #     {'kernel': ['linear'], 'C': [1, 10]},\n",
    "    #     {'kernel': ['rbf'], 'C': [1, 10], 'gamma': [0.001, 0.0001]},\n",
    "    # ],\n",
    "}\n",
    "\n",
    "\n",
    "INPUT_PICKLE_FILE = True\n",
    "INPUT_DIR = 'COMBINED_DATA_TOUCHTALE'\n",
    "INPUT_PICKLE_NAME = '_featurized_data.pk'\n",
    "INPUT_LABEL_NAME = '_labels.pk'\n",
    "\n",
    "INPUT_PICKLE_NAME_VAL = '_val_featurized_data.pk'\n",
    "INPUT_LABEL_NAME_VAL = '_val_labels.pk'\n",
    "\n",
    "SAVE_PICKLE_FILE = True\n",
    "OUTPUT_DIR = 'RESULTS'\n",
    "OUTPUT_PICKLE_NAME = 'results.pk'\n",
    "# print(\"hello\")\n",
    "\n",
    "# LABEL_TYPES = ['pos', 'angle', 'acc', 'cw']\n",
    "LABEL_TYPES = ['pos', 'angle', 'acc']\n",
    "\n",
    "\"\"\"\n",
    "Modified from: https://gist.github.com/DimaK415/428bbeb0e79551f780bb990e7c26f813\n",
    "\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "CV_SPLITS = 3\n",
    "\n",
    "\n",
    "class EstimatorSelectionHelper:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\n",
    "                \"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "        self.rfes = {}\n",
    "\n",
    "    def cv(self, n_splits=CV_SPLITS, random_state=0, shuffle=True):\n",
    "        return StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=shuffle)\n",
    "\n",
    "    def rfe(self, clf, cv=None, step=1, scoring='f1_macro'):\n",
    "        if cv is None:\n",
    "            cv = self.cv()\n",
    "        return RFECV(estimator=clf, step=step, cv=cv, scoring=scoring)\n",
    "\n",
    "    def pipeline(self, rfecv, gs_cv):\n",
    "        return Pipeline([('feature_sele', rfecv), ('clf_cv', gs_cv)])\n",
    "\n",
    "    def fit(self, X, y, cv=None, n_jobs=3, verbose=1, scoring=None, refit=False, pnum='p01', label='pos', window_size='5000ms', val_X=None, val_y=None):\n",
    "        if cv is None:\n",
    "            cv = self.cv()\n",
    "        \n",
    "        f, axes = plt.subplots(1, len(MODELS), figsize=(len(MODELS)*2.5, 3), sharey='row')\n",
    "\n",
    "        for i, key in enumerate(self.keys):\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            rfecv = self.rfe(model)\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring, refit=refit,\n",
    "                              return_train_score=True)\n",
    "            # print(gs.best_params_)\n",
    "            # y_pred = gs.best_estimator_.predict(X)\n",
    "            # print(confusion_matrix(y, y_pred))\n",
    "            pipe = self.pipeline(rfecv, gs)\n",
    "            pipe.fit(X, y)\n",
    "\n",
    "\n",
    "            # print(pipe._final_estimator.best_params_)\n",
    "            self.y_pred = pipe.predict(val_X)\n",
    "\n",
    "            cm = confusion_matrix(val_y, self.y_pred)\n",
    "            # print(f\"{key}: \", cm)\n",
    "\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "            disp.plot(ax=axes[i], xticks_rotation=45)\n",
    "            disp.ax_.set_title(key)\n",
    "            disp.im_.colorbar.remove()\n",
    "            disp.ax_.set_xlabel('')\n",
    "            if i != 0:\n",
    "                disp.ax_.set_ylabel('')\n",
    "\n",
    "            self.grid_searches[key] = gs\n",
    "            self.rfes[key] = rfecv\n",
    "\n",
    "        f.suptitle(f'Confusion Matrix for {pnum}-{label}-{window_size}')\n",
    "        f.text(0.4, 0.1, 'Predicted Label', ha='left')\n",
    "        # f.text(0.04, 0.5, 'True Label', va='center', rotation='vertical')\n",
    "\n",
    "        plt.subplots_adjust(wspace=0.8)\n",
    "\n",
    "        f.colorbar(disp.im_, ax=axes)\n",
    "        plt.savefig(f'images/cm/cm-{pnum}-{label}-{window_size}.png')\n",
    "\n",
    "\n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                'estimator': key,\n",
    "                'min_score': min(scores),\n",
    "                'max_score': max(scores),\n",
    "                'mean_score': np.mean(scores),\n",
    "                'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params, **d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "\n",
    "            keys = self.grid_searches[k].cv_results_.keys()\n",
    "            # print(\"keys: \", keys)\n",
    "            for i in range(self.grid_searches[k].cv.get_n_splits()):\n",
    "                key = \"split{}_test_f1\".format(i)\n",
    "                if key in keys:\n",
    "                    r = self.grid_searches[k].cv_results_[key]\n",
    "                    scores.append(r.reshape(len(params), 1))\n",
    "                else:\n",
    "                    # print(f\"key: {key}, keys: {keys}\")\n",
    "                    pass\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            try:\n",
    "                print(f\"all score {all_scores}, params {params}, type: {type(all_scores)}/{type(params)}\")\n",
    "                \n",
    "                for p, s in zip(params, all_scores):\n",
    "                    rows.append((row(k, s, p)))\n",
    "            except Exception as e:\n",
    "                print(f\"147 all score {all_scores}, params {params}\")\n",
    "                print(\"error! \", e)\n",
    "\n",
    "        # print(\"151\", pd.concat(rows, axis=1).T)\n",
    "\n",
    "        # print(pd.concat(rows, axis=1).T.sort_values(by=['mean_score'], ascending=False))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values(by=[sort_by], ascending=False)\n",
    "        # print(\"152\")\n",
    "\n",
    "        columns = ['estimator', 'min_score',\n",
    "                   'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        # print(\"158, s\", columns)\n",
    "\n",
    "        return df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f007bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training participant p_all, 2000\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(feature_dict, label_dict, label_types, pnum, window_size, val_feature_dict, val_label_dict)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m     features \u001b[39m=\u001b[39m feature_dict\u001b[39m.\u001b[39;49mdrop(\u001b[39m'\u001b[39;49m\u001b[39mtimedelta\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39mto_numpy()\n\u001b[1;32m     37\u001b[0m     \u001b[39m# val_features = val_feature_dict.drop('timedelta', axis=1).to_numpy()\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5262\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   5263\u001b[0m \u001b[39mDrop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5264\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5397\u001b[0m \u001b[39m        weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5398\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 5399\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[1;32m   5400\u001b[0m     labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   5401\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   5402\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   5403\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m   5404\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m   5405\u001b[0m     inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[1;32m   5406\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   5407\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4504\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 4505\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   4507\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4545\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 4546\u001b[0m     new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   4547\u001b[0m indexer \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mget_indexer(new_axis)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6933\u001b[0m \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 6934\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(labels[mask])\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6935\u001b[0m indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['timedelta'] not found in axis\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 146\u001b[0m\n\u001b[1;32m    143\u001b[0m val_labels \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mload_pickle(pickled_file_path\u001b[39m=\u001b[39minput_val_label_file_path)\n\u001b[1;32m    145\u001b[0m utils\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mWindow size \u001b[39m\u001b[39m{\u001b[39;00mwindow_size\u001b[39m}\u001b[39;00m\u001b[39m - Iteration \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 146\u001b[0m training_results \u001b[39m=\u001b[39m train(features[subject_id], labels[subject_id], pnum\u001b[39m=\u001b[39;49msubject_id, window_size\u001b[39m=\u001b[39;49mwindow_size, val_feature_dict\u001b[39m=\u001b[39;49mval_features, val_label_dict\u001b[39m=\u001b[39;49mval_labels)\n\u001b[1;32m    148\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mresults\u001b[39m\u001b[39m\"\u001b[39m, training_results)\n\u001b[1;32m    149\u001b[0m \u001b[39mif\u001b[39;00m SAVE_PICKLE_FILE:\n",
      "Cell \u001b[0;32mIn[3], line 42\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(feature_dict, label_dict, label_types, pnum, window_size, val_feature_dict, val_label_dict)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     41\u001b[0m     features \u001b[39m=\u001b[39m feature_dict\u001b[39m.\u001b[39mto_numpy()\n\u001b[0;32m---> 42\u001b[0m     val_features \u001b[39m=\u001b[39m val_feature_dict\u001b[39m.\u001b[39;49mto_numpy()\n\u001b[1;32m     43\u001b[0m     \u001b[39mprint\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mno time_delta\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     45\u001b[0m X \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "TYPE_TO_TRAIN = 'cw_mode'\n",
    "\n",
    "def fit_helper(X, y, pnum='p01', label='pos', window_size='5000ms', val_X=None, val_y=None, models=MODELS, params=PARAMS, n_jobs=-1, \n",
    "               scoring={ \"f1\": make_scorer(metrics.f1), \"prec\" : make_scorer(metrics.precision), \"recall\": make_scorer(metrics.recall)}, cw_classes=None):\n",
    "\n",
    "    # \"confusionMat\": make_scorer(metrics.confusion_matrix) make_scorer(fbeta_score, beta=2)\n",
    "    # print(models, params, cw_classes)\n",
    "    helper = EstimatorSelectionHelper(models, params)\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    if len(unique) < 2:\n",
    "        print(y)\n",
    "        print(\"Not enough label classes! \")\n",
    "        return -1, -1 \n",
    "    helper.fit(X, y, scoring=scoring, n_jobs=n_jobs, refit='f1', pnum=pnum, label=label, window_size=window_size, val_X=val_X, val_y=val_y)\n",
    "    try:\n",
    "        scores = helper.score_summary()\n",
    "        return helper, scores\n",
    "    except ValueError as e:\n",
    "        print(\"Value error! \", e)\n",
    "        return -1, -1 \n",
    "\n",
    "\n",
    "def train(feature_dict, label_dict, label_types=LABEL_TYPES, pnum='p01', window_size='5000ms', val_feature_dict=None, val_label_dict=None):\n",
    "\n",
    "    # read the features back into a numpy array\n",
    "    # bands = feature_dict[feature_types].to_numpy()\n",
    "    # features = np.empty((bands.shape[0], bands.shape[1], 64, 64))\n",
    "    # for band in range(len(feature_types)):\n",
    "    #     band_arr = bands[:,band]\n",
    "    #     for window in range(band_arr.shape[0]):\n",
    "    #         band_im = np.array(band_arr[window])\n",
    "    #         features[window, band, :, :] = band_im\n",
    "\n",
    "    # print(feature_dict)\n",
    "        print(val_feature_dict)\n",
    "    \n",
    "    try:\n",
    "        features = feature_dict.drop('timedelta', axis=1).to_numpy()\n",
    "        # val_features = val_feature_dict.drop('timedelta', axis=1).to_numpy()\n",
    "        print(val_feature_dict)\n",
    "        \n",
    "    except Exception as e:\n",
    "        features = feature_dict.to_numpy()\n",
    "        val_features = val_feature_dict.to_numpy()\n",
    "        print(e, 'no time_delta')\n",
    "\n",
    "    X = features.astype(np.float32)\n",
    "    val_X = val_features.astype(np.float32)\n",
    "\n",
    "    print(f\"train feature dim: {feature_dict.shape}, train label dim: {label_dict.shape}\")\n",
    "    print(f\"train feature dim: {val_feature_dict.shape}, train label dim: {val_label_dict.shape}\")\n",
    "\n",
    "    # print(X.shape)\n",
    "\n",
    "    res = {}\n",
    "   \n",
    "    res['pnum'] = pnum\n",
    "\n",
    "    # Y = label_dict.loc[label_dict['window_id'].isin(feature_dict['window_id'].unique())]\n",
    "    \n",
    "    Y = label_dict\n",
    "    val_Y = val_label_dict\n",
    "    # return\n",
    "\n",
    "    LE = LabelEncoder() # transform string to class values\n",
    "    LE.fit(Y[TYPE_TO_TRAIN])\n",
    "\n",
    "    y_cw_str = Y[TYPE_TO_TRAIN]\n",
    "    y_cw = LE.transform(y_cw_str)\n",
    "\n",
    "    print(\"Does X have nan? \", np.isnan(X).any(), np.count_nonzero(np.isnan(X)))\n",
    "\n",
    "    LE_val = LabelEncoder() # transform string to class values\n",
    "    LE_val.fit(val_Y[TYPE_TO_TRAIN])\n",
    "\n",
    "    val_y_cw_str = val_Y[TYPE_TO_TRAIN]\n",
    "    val_y_cw = LE.transform(val_y_cw_str)\n",
    "\n",
    "    if np.isnan(X).any():\n",
    "        mask = np.isnan(X)\n",
    "        idx = np.where(~mask,np.arange(mask.shape[1]),0)\n",
    "        np.maximum.accumulate(idx, axis=1, out=idx)\n",
    "        X_filtered = X[np.arange(idx.shape[0])[:, None], idx]\n",
    "    else:\n",
    "        X_filtered = X\n",
    "\n",
    "    for label_type in label_types: # train every label type\n",
    "        print(f'Training label type {label_type}')\n",
    "\n",
    "        # y = np.array(list(zip(y_cw, Y[label_type])))\n",
    "        # y_str = np.array(list(zip(y_cw_str, Y[label_type] )))\n",
    "\n",
    "        # y = np.array(y_cw)\n",
    "        y_str = np.array(y_cw_str)\n",
    "        val_y_str = np.array(val_y_cw_str)\n",
    "\n",
    "        # print(y_cw)\n",
    "        # print(\"shapeee\", X.shape, Y[label_type].shape)\n",
    "        # return\n",
    "\n",
    "        y = LE.fit_transform(Y[label_type])\n",
    "        val_y = LE.fit_transform(val_Y[label_type])\n",
    "\n",
    "        res['y_hc_' + label_type] = y_str\n",
    "\n",
    "        print(\"Does y have nan? \", np.isnan(y).any(), np.count_nonzero(np.isnan(y)))\n",
    "        if np.isnan(y).any():\n",
    "            mask = np.isnan(y)\n",
    "            idx = np.where(~mask,np.arange(mask.shape[1]),0)\n",
    "            np.maximum.accumulate(idx, axis=1, out=idx)\n",
    "            y_filtered = y[np.arange(idx.shape[0])[:, None], idx]\n",
    "        else:\n",
    "            y_filtered = y\n",
    "\n",
    "        # print(sorted(y_filtered), np.array(y_filtered).shape)\n",
    "\n",
    "        helper, scores = fit_helper(X_filtered, y_filtered, pnum=pnum, label=label_type, window_size=window_size, val_X=val_X, val_y=val_y)\n",
    "        res['scores_hc_' + label_type] = scores\n",
    "        del helper\n",
    "        del scores\n",
    "\n",
    "    return res\n",
    "\n",
    "for i in range(0, 1):\n",
    "    for window_size in EXP_PARAMS['WINDOW_SIZE']:\n",
    "        if INPUT_PICKLE_FILE:\n",
    "            participant_results = {}\n",
    "            for subject_id in tqdm(SUBJECT_IDS):\n",
    "                # utils.logger.info(f'Training participant {subject_id}')\n",
    "\n",
    "                print(f'Training participant {subject_id}, {window_size}')\n",
    "\n",
    "                training_data_filename = subject_id + \"_\" + str(window_size) + 'ms' + INPUT_PICKLE_NAME\n",
    "                input_pickle_file_path = os.path.join(INPUT_DIR, training_data_filename)\n",
    "                input_label_file_path = os.path.join(INPUT_DIR, str(window_size) + 'ms' + INPUT_LABEL_NAME)\n",
    "\n",
    "                features = utils.load_pickle(pickled_file_path=input_pickle_file_path)\n",
    "                labels = utils.load_pickle(pickled_file_path=input_label_file_path)\n",
    "\n",
    "                val_data_filename = subject_id + \"_\" + str(window_size) + 'ms' + INPUT_PICKLE_NAME_VAL\n",
    "                input_val_pickle_file_path = os.path.join(INPUT_DIR, val_data_filename)\n",
    "                input_val_label_file_path = os.path.join(INPUT_DIR, str(window_size) + 'ms' + INPUT_LABEL_NAME_VAL)\n",
    "\n",
    "                val_features = utils.load_pickle(pickled_file_path=input_val_pickle_file_path)\n",
    "                val_labels = utils.load_pickle(pickled_file_path=input_val_label_file_path)\n",
    "\n",
    "                utils.logger.info(f'Window size {window_size} - Iteration {i}')\n",
    "                training_results = train(features[subject_id], labels[subject_id], pnum=subject_id, window_size=window_size, val_feature_dict=val_features, val_label_dict=val_labels)\n",
    "                \n",
    "                print(\"results\", training_results)\n",
    "                if SAVE_PICKLE_FILE:\n",
    "                    iter_ = subject_id + \"_\" + str(i) + '_' + str(window_size) + 'ms_hc_cw_' + OUTPUT_PICKLE_NAME\n",
    "                    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "                    output_pickle_file_path = os.path.join(OUTPUT_DIR, iter_)\n",
    "                    utils.pickle_data(data=training_results, file_path=output_pickle_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/emod-eegify.nosync/src/RESULTS/'\n",
    "INPUT_PICKLE_NAME = 'results.pk'\n",
    "\n",
    "cols_to_print = ['estimator', 'mean_score', 'std_score']\n",
    "\n",
    "labels_to_plot = ['scores_hc_pos', 'scores_hc_angle', 'scores_hc_acc']\n",
    "labels_to_print = [label.split('_')[-1] for label in labels_to_plot]\n",
    "# labels_to_plot = ['scores_hc_acc']\n",
    "score_to_plot = 'mean_score'\n",
    "# cols_to_print = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "\n",
    "def setBoxColors(bp):\n",
    "    # print(bp)\n",
    "    setp(bp['boxes'][0], color='blue')\n",
    "    setp(bp['caps'][0], color='blue')\n",
    "    setp(bp['caps'][1], color='blue')\n",
    "    setp(bp['whiskers'][0], color='blue')\n",
    "    setp(bp['whiskers'][1], color='blue')\n",
    "    setp(bp['fliers'][0], markeredgecolor='grey')\n",
    "    setp(bp['medians'][0], color='black')\n",
    "\n",
    "    setp(bp['boxes'][1], color='red')\n",
    "    setp(bp['caps'][2], color='red')\n",
    "    setp(bp['caps'][3], color='red')\n",
    "    setp(bp['whiskers'][2], color='red')\n",
    "    setp(bp['whiskers'][3], color='red')\n",
    "    setp(bp['fliers'][1], markeredgecolor='grey')\n",
    "    setp(bp['medians'][1], color='black')\n",
    "\n",
    "    setp(bp['boxes'][2], color='green')\n",
    "    setp(bp['caps'][3], color='green')\n",
    "    setp(bp['caps'][4], color='green')\n",
    "    setp(bp['whiskers'][3], color='green')\n",
    "    setp(bp['whiskers'][4], color='green')\n",
    "    setp(bp['fliers'][2], markeredgecolor='grey')\n",
    "    setp(bp['medians'][2], color='black')\n",
    "\n",
    "# # set axes limits and labels\n",
    "# xlim(0,9)\n",
    "# ylim(0,9)\n",
    "# ax.set_xticklabels(['A', 'B', 'C'])\n",
    "# ax.set_xticks([1.5, 4.5, 7.5])\n",
    "\n",
    "# # draw temporary red and blue lines and use them to create a legend\n",
    "# hB, = plot([1,1],'b-')\n",
    "# hR, = plot([1,1],'r-')\n",
    "# legend((hB, hR),('Apples', 'Oranges'))\n",
    "# hB.set_visible(False)\n",
    "# hR.set_visible(False)\n",
    "\n",
    "# savefig('boxcompare.png')\n",
    "# show()\n",
    "\n",
    "YMAX = 1\n",
    "YMIN = 0.2\n",
    "XMIN = -2\n",
    "\n",
    "for i, window_size in enumerate(EXP_PARAMS[\"WINDOW_SIZE\"]):\n",
    "    data_to_plot = {}\n",
    "\n",
    "    fig = figure(figsize=(8, 6))\n",
    "    ax = axes()\n",
    "\n",
    "    count = 0\n",
    "    midpoints = []\n",
    "    for i, subject_id in enumerate(SUBJECT_IDS):\n",
    "        input_pickle_file_path = os.path.join(INPUT_DIR, subject_id + \"_\" + str(0) + '_' + str(window_size) + 'ms_hc_cw_' + INPUT_PICKLE_NAME)\n",
    "        res = utils.load_pickle(pickled_file_path=input_pickle_file_path)\n",
    "\n",
    "        data_to_plot[res['pnum']] = []\n",
    "        for label in labels_to_plot:\n",
    "            data_to_plot[res['pnum']].append(res[label][score_to_plot].to_list())\n",
    "\n",
    "\n",
    "        # print(np.array(data_to_plot[res['pnum']]).shape)\n",
    "        bp = boxplot(np.array(data_to_plot[res['pnum']]).transpose(), positions=np.arange(count, count+3), widths = 0.8)\n",
    "        setBoxColors(bp)\n",
    "\n",
    "        midpoints.append(count+1)\n",
    "\n",
    "        if i != len(SUBJECT_IDS)-1:\n",
    "            ax.vlines(x=count+3.5, ymin=YMIN, ymax=YMAX, colors='orange', ls='--', lw=1, alpha=0.5)\n",
    "        count += 5\n",
    "\n",
    "    ax.set_title(f\"Cross Validation Mean Score {'-'.join(labels_to_print)}\\n for Window Size {window_size}ms\")\n",
    "    ax.set_xlabel(f\"Participant ID\")\n",
    "    ax.set_ylabel(f\"{score_to_plot}\")\n",
    "    \n",
    "    ylim(YMIN, YMAX)\n",
    "    xlim(XMIN, count)\n",
    "\n",
    "    # print(midpoints, SUBJECT_IDS)\n",
    "\n",
    "    hB, = plot([1,1],'b-')\n",
    "    hR, = plot([1,1],'r-')\n",
    "    hG, = plot([1,1],'g-')\n",
    "    legend((hB, hR, hG), ('position', 'angle', 'accumulator'))\n",
    "    hB.set_visible(False)\n",
    "    hR.set_visible(False)\n",
    "    hG.set_visible(False)\n",
    "\n",
    "    ax.xaxis.set_ticks(midpoints, sorted(SUBJECT_IDS))\n",
    "    ax.set_xticks(midpoints)\n",
    "    show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283228ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd3f8c78",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40983ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_features=4, random_state=0)\n",
    "print(X.shape, y.shape)\n",
    "clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
    "clf.fit(X, y)\n",
    "clf.predict([[0, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a3e503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790a12a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
