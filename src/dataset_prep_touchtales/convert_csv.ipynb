{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/poyuchen/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/poyuchen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import sklearn\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser\n",
    "from scipy.signal import savgol_filter\n",
    "from ydata_profiling import ProfileReport\n",
    "from arabica import arabica_freq, cappuccino, coffee_break\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy.signal import argrelextrema, find_peaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TouchtalesPipeline(): \n",
    "    def __init__(self, dataset_name, task_one_ds, task_two_ds, task_three_ds, task_four_ds, task_five_ds, transcript_ds, start_timestamp, video_start, profiling_report_dir, output_dir):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.task_one_ds = task_one_ds              # Path to data source for task 1 data\n",
    "        self.task_two_ds = task_two_ds              # Path to data source for task 2 data\n",
    "        self.task_three_ds = task_three_ds          # Path to data source for task 3 data\n",
    "        self.task_four_ds = task_four_ds            # Path to data source for task 4 data\n",
    "        self.task_five_ds = task_five_ds            # Path to data source for task 5 data\n",
    "        self.transcript_ds = transcript_ds          # Path to data source for transcript\n",
    "        self.start_timestamp = start_timestamp      # Start timestamp for data collection\n",
    "        self.video_start = video_start              # Start timestamp for video\n",
    "        self.merged_data = pd.DataFrame()\n",
    "        self.profiling_report_dir = profiling_report_dir    # Directory for output profiling report\n",
    "        self.output_dir = output_dir                # Directory for output csv\n",
    "        self.calibration_range = [450, -50]\n",
    "        self.feeltrace_range = [700, 400]\n",
    "\n",
    "        path = Path(self.output_dir)\n",
    "        # print(path.absolute())\n",
    "        Path(path.parent.absolute()).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Main function for cleaning and merging data streams\n",
    "    def clean_data(self): \n",
    "        self.get_task_one()\n",
    "        self.get_task_two()\n",
    "        self.get_task_five()\n",
    "        self.get_task_four()\n",
    "        self.get_task_three()\n",
    "        \n",
    "        self.convert_transcript()\n",
    "        self.synchronize_transcript()\n",
    "        self.merge_data()\n",
    "        # profile = ProfileReport(self.merged_data, title=\"Touchtales Profiling Report\")\n",
    "        # profile.to_file(self.profiling_report_dir)\n",
    "        self.merged_data.to_csv(self.output_dir)\n",
    "\n",
    "        # self.merged_data.set_index('timestamp').plot()\n",
    "        # plt.xlabel(\"Timestamp\")\n",
    "        # plt.ylabel(\"Biosignal Value\")\n",
    "        # plt.title(\"Biosignal Data Over Time\") \n",
    "        # plt.show()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    # Return a cleaned dataframe for task one data (biosignal)\n",
    "    def get_task_one(self):\n",
    "        with open(self.task_one_ds, \"r\") as file: \n",
    "            data = file.read().rstrip()\n",
    "        \n",
    "        data = json.loads(data)\n",
    "        self.task_one_df = pd.DataFrame(data)\n",
    "        self.task_one_df['timestamp'] = pd.to_datetime(self.task_one_df[\"timestamp\"], unit='ms')\n",
    "        self.clean_task_one()\n",
    "        return\n",
    "    \n",
    "    # Clean task one data, return cleaned dataframe as a class variable, line up to start timestamp\n",
    "    def clean_task_one(self):\n",
    "        # Smoothing task 1 biosignals\n",
    "        self.task_one_df[\"gsr_smoothed\"] = savgol_filter(self.task_one_df[\"gsr\"], 100, 1)\n",
    "        self.task_one_df[\"bpm_smoothed\"] = savgol_filter(self.task_one_df[\"bpm\"], 100, 1)\n",
    "        pds = pd.Series(~(self.task_one_df['timestamp'] < self.start_timestamp))\n",
    "        self.task_one_df = self.task_one_df[~(self.task_one_df['timestamp'] < self.start_timestamp)]\n",
    "        self.task_one_df = self.task_one_df.drop(columns=['resp', 'feeltrace', 'comment'])\n",
    "        return\n",
    "\n",
    "\n",
    "    def plot_uncleaned_task_one(self):\n",
    "        print(len(self.task_one_df[\"bpm\"]), len(self.task_one_df[\"bpm_smoothed\"]))\n",
    "        plt.plot(self.task_one_df[\"bpm\"])\n",
    "        plt.plot(self.task_one_df[\"bpm_smoothed\"])\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "\n",
    "    # Get cleaned task two data\n",
    "    def get_task_two(self):\n",
    "        with open(self.task_two_ds, \"r\") as file: \n",
    "            data = file.read().rstrip()\n",
    "        \n",
    "        data = json.loads(data)\n",
    "        self.task_two_df = pd.DataFrame(data[:-1])[[\"label\", \"y\"]]\n",
    "        axis = data[-1][\"axis\"] # First item is higher, second item is lower\n",
    "        row_high_axis = {'label': axis[0], 'y': self.calibration_range[0]}\n",
    "        row_low_axis = {'label': axis[1], 'y': self.calibration_range[1]}\n",
    "        self.task_two_df = pd.concat([self.task_two_df, pd.DataFrame([row_high_axis])], ignore_index=True)\n",
    "        self.task_two_df = pd.concat([self.task_two_df, pd.DataFrame([row_low_axis])], ignore_index=True)\n",
    "        self.task_two_df = self.task_two_df.sort_values(by = [\"y\"], ascending=True)\n",
    "        return\n",
    "    \n",
    "    # Get cleaned task three data (interview)\n",
    "    def get_task_three(self):\n",
    "        with open(self.task_three_ds, 'r') as file:\n",
    "            data = file.read().rstrip()\n",
    "\n",
    "        data = json.loads(data)\n",
    "        self.task_three_df = pd.DataFrame(data)\n",
    "        self.task_three_df['timestamp'] = pd.to_timedelta(self.task_three_df['timestamp'], unit='s', errors='coerce')\n",
    "\n",
    "        self.clean_task_three()\n",
    "        return\n",
    "    \n",
    "    # Clean task three data, return cleaned data as a class variable, line up to video timestamp\n",
    "    def clean_task_three(self):\n",
    "        self.task_three_df = self.task_three_df[~(self.task_three_df['timestamp'] < self.video_start)]\n",
    "        self.task_three_df['timestamp'] = self.task_three_df.apply(lambda x: x['timestamp'] + self.start_timestamp, axis=1)\n",
    "        return\n",
    "    \n",
    "    # Get cleaned task four data (feeltrace)\n",
    "    def get_task_four(self):\n",
    "        with open(self.task_four_ds, 'r') as file:\n",
    "            data = file.read().rstrip()\n",
    "\n",
    "        data = json.loads(data)\n",
    "        self.task_four_df = pd.DataFrame(data)\n",
    "        self.task_four_df = self.task_four_df.drop(columns=['touch', 'bpm', 'gsr', 'comment', 'resp'])\n",
    "        self.task_four_df['timestamp'] = pd.to_datetime(self.task_four_df[\"timestamp\"], unit='ms', errors='coerce')\n",
    "        self.clean_task_four()\n",
    "\n",
    "        self.feeltrace_range = [self.task_four_df[\"feeltrace_cleaned\"].max(), self.task_four_df[\"feeltrace_cleaned\"].min()]\n",
    "        self.add_calibration_to_feeltrace(self.task_two_df, \"calibration_1\")\n",
    "        self.add_calibration_to_feeltrace(self.task_five_df, \"calibration_2\")\n",
    "        return\n",
    "    \n",
    "    def add_calibration_to_feeltrace(self, task_df, col_name):\n",
    "        feeltrace_span = self.feeltrace_range[0] - self.feeltrace_range[1]\n",
    "        calibration_span = self.calibration_range[0] - self.calibration_range[1]\n",
    "        task_df[\"rescaled_df\"] = self.feeltrace_range[1] + ((task_df[\"y\"] - self.calibration_range[1])/calibration_span)*feeltrace_span\n",
    "        # print(task_df)\n",
    "        self.task_four_df[col_name] = self.task_four_df.apply(lambda z: task_df[\"label\"][min(enumerate(list(task_df[\"y\"])), key=lambda x: abs(x[1]-z[\"feeltrace_cleaned\"]))[0]], axis=1)\n",
    "        return\n",
    "    \n",
    "    # Clean tasxk four data (feeltrace)\n",
    "    def clean_task_four(self):\n",
    "        df = self.task_four_df\n",
    "        delta = 0.03\n",
    "        span = 10\n",
    "        df['y_ewma_fb'] = self.ewma_fb(df['feeltrace'], span)\n",
    "        df['y_remove_outliers'] = self.remove_outliers(df['feeltrace'].tolist(), df['y_ewma_fb'].tolist(), delta)\n",
    "        df['feeltrace_cleaned'] = df['y_remove_outliers'].interpolate()\n",
    "           \n",
    "        self.task_four_df = df.drop(columns=[\"y_ewma_fb\", \"y_remove_outliers\"])\n",
    "        min_time = self.task_four_df[\"timestamp\"].min()\n",
    "        self.task_four_df = self.task_four_df[~(self.task_four_df['timestamp'] < min_time + self.video_start)]\n",
    "        min_time = self.task_four_df[\"timestamp\"].min()\n",
    "        time_delta = min_time - self.start_timestamp\n",
    "        self.task_four_df['timestamp'] = self.task_four_df.apply(lambda x: x['timestamp'] - time_delta, axis=1)\n",
    "        \n",
    "        return \n",
    "\n",
    "    # Apply forwards, backwards exponential weighted moving average (EWMA) to a column in a dataframe (df_column)\n",
    "    def ewma_fb(self, df_column, span):\n",
    "        # Forwards EWMA.\n",
    "        fwd = pd.Series.ewm(df_column, span=span).mean()\n",
    "        # Backwards EWMA.\n",
    "        bwd = pd.Series.ewm(df_column[::-1],span=10).mean()\n",
    "        # Add and take the mean of the forwards and backwards EWMA.\n",
    "        stacked_ewma = np.vstack(( fwd, bwd[::-1] ))\n",
    "        fb_ewma = np.mean(stacked_ewma, axis=0)\n",
    "        return fb_ewma\n",
    "        \n",
    "    # Remove data from noisy column that is > delta from fbewma. \n",
    "    def remove_outliers(self, noisy, fbewma, delta):\n",
    "        np_noisy = np.array(noisy)\n",
    "        np_fbewma = np.array(fbewma)\n",
    "        cond_delta = (np.abs(np_noisy-np_fbewma) > delta)\n",
    "        np_remove_outliers = np.where(cond_delta, np.nan, np_noisy)\n",
    "        return np_remove_outliers\n",
    "            \n",
    "    # Get cleaned task five data\n",
    "    def get_task_five(self):\n",
    "        with open(self.task_five_ds, \"r\") as file: \n",
    "            data = file.read().rstrip()\n",
    "        \n",
    "        data = json.loads(data)\n",
    "        self.task_five_df = pd.DataFrame(data[:-1])[[\"label\", \"y\"]]\n",
    "        axis = data[-1][\"axis\"] # First item is higher, second item is lower\n",
    "        row_high_axis = {'label': axis[0], 'y': self.calibration_range[0]}\n",
    "        row_low_axis = {'label': axis[1], 'y': self.calibration_range[1]}\n",
    "        self.task_five_df = pd.concat([self.task_five_df, pd.DataFrame([row_high_axis])], ignore_index=True)\n",
    "        self.task_five_df = pd.concat([self.task_five_df, pd.DataFrame([row_low_axis])], ignore_index=True)\n",
    "        self.task_five_df = self.task_five_df.sort_values(by = [\"y\"], ascending=True)\n",
    "        \n",
    "        return\n",
    "\n",
    "    # Convert transcript to a readable csv\n",
    "    def convert_transcript(self):\n",
    "        input_vtt = self.transcript_ds\n",
    "        opened_file = open(input_vtt, encoding='utf8')\n",
    "        content = opened_file.read()\n",
    "        segments = content.split('\\n\\n') # split on double line\n",
    "        # wrangle segments\n",
    "        m = re.compile(r\"\\<.*?\\>\") # strip/remove unwanted tags\n",
    "        new_segments = [self.clean_transcript_line(s, m) for s in segments if len(s)!=0][1:]\n",
    "\n",
    "        trimmed_segments = []\n",
    "        for segment in new_segments:\n",
    "            split_segment = segment.split()\n",
    "            time_code = split_segment[0]\n",
    "            text = ' '.join(segment.split()[1:])\n",
    "            trimmed_segment = (time_code, str(text[:12]), str(text[13:24]), str(text[25:]))\n",
    "            trimmed_segments.append(trimmed_segment)\n",
    "        \n",
    "        # Add trimmed segments to csv\n",
    "        with open(str(input_vtt)[:-3]+'csv', 'w', encoding='utf8', newline='') as f:\n",
    "            for line in trimmed_segments:\n",
    "                thewriter = csv.writer(f)\n",
    "                thewriter.writerow(line)\n",
    "\n",
    "        self.transcript_ds = self.transcript_ds[:-3] + 'csv'\n",
    "        return\n",
    "\n",
    "    # Clean a single line of the transcript file\n",
    "    def clean_transcript_line(self, content, m):\n",
    "        new_content = m.sub('',content)\n",
    "        # new_content = o.sub('',new_content)\n",
    "        new_content = new_content.replace('align:start position:0%','')\n",
    "        new_content = new_content.replace('-->','')\n",
    "        return new_content\n",
    "    \n",
    "    # trim time codes for g suite plain text formatting conversion to seconds w/ formula '=value(str*24*3600)'\n",
    "    def clean_time(time):\n",
    "        time = time.split(':')\n",
    "        if time[0]=='00':\n",
    "            return time[1]+':'+time[2]\n",
    "        if not time[0]=='00':\n",
    "            return time[0]+':'+time[1]+':'+time[2]\n",
    "\n",
    "    # Synchronize transcript CSV with preexisting biosignal and feeltrace data\n",
    "    def synchronize_transcript(self):\n",
    "        colnames = [\"time_id\", \"transcript_timestamp_start\", \"transcript_timestamp_finish\", \"transcript\"]\n",
    "        self.transcript_df = pd.read_csv(self.transcript_ds, names = colnames, header = None)\n",
    "        # self.transcript_df[\"duration\"] = self.transcript_df.apply(lambda x: str(x[\"Duration\"][0:-3] + \".\" + x[\"Duration\"][-2:]), axis=1)\n",
    "        self.transcript_df[\"transcript_timestamp_start\"] = pd.to_timedelta(self.transcript_df['transcript_timestamp_start'])\n",
    "        self.transcript_df[\"transcript_timestamp_finish\"] = pd.to_timedelta(self.transcript_df['transcript_timestamp_finish'])\n",
    "        self.transcript_df[\"transcript_duration\"] = (self.transcript_df['transcript_timestamp_finish']- self.transcript_df['transcript_timestamp_start']).fillna(pd.Timedelta(0))\n",
    "        self.transcript_df = self.transcript_df[~(self.transcript_df['transcript_timestamp_start'] < self.video_start)]\n",
    "        self.transcript_df['timestamp'] = self.transcript_df.apply(lambda x: x['transcript_timestamp_start'] + self.start_timestamp, axis=1)\n",
    "        self.transcript_df['transcript_timestamp_finish'] = self.transcript_df.apply(lambda x: x['transcript_timestamp_finish'] + self.start_timestamp, axis=1)\n",
    "        # self.transcript_df[\"transcript_timestamp_start\"] = self.transcript_df['timestamp']\n",
    "\n",
    "        self.transcript_df = self.transcript_df.drop(columns=[\"transcript_timestamp_start\", \"time_id\"])\n",
    "        return\n",
    "    \n",
    "    def merge_data(self): \n",
    "        self.merged_data = pd.merge_asof(self.task_one_df, self.task_four_df, on=\"timestamp\", tolerance=pd.Timedelta('1s'))\n",
    "        self.merged_data = pd.merge_asof(self.merged_data, self.task_three_df, on=\"timestamp\", tolerance=pd.Timedelta('1s'))\n",
    "        self.merged_data = pd.merge_asof(self.merged_data, self.transcript_df, on=\"timestamp\", tolerance=pd.Timedelta('1s'))\n",
    "        self.merged_data = self.merged_data[self.merged_data['feeltrace'].notna()]\n",
    "        self.merged_data[\"timestamp\"] = self.merged_data[\"timestamp\"].apply(lambda x: datetime.timestamp(x))\n",
    "        # self.merged_data = self.merged_data['transcript_tokenized']\n",
    "        print(self.merged_data.columns)\n",
    "    \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"dataset_name\": \"poyuchen\",\n",
      "    \"date\": \"chen\",\n",
      "    \"task_one_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/iag0326/iag0326_TaskOne_752.txt\",\n",
      "    \"task_two_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/iag0326/iag0326_TaskTwo_1679872661016.txt\",\n",
      "    \"task_three_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/iag0326/iag0326_TaskThree_1679874475963.txt\",\n",
      "    \"task_four_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/iag0326/iag0326_TaskFour_1679875274351.txt\",\n",
      "    \"task_five_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/iag0326/iag0326_TaskFive_1679875613385.txt\",\n",
      "    \"transcript_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/iag0326/transcript.vtt\",\n",
      "    \"start_timestamp\": \"2023-03-26 22:53:58.0\",\n",
      "    \"video_start\": \"10\",\n",
      "    \"output_dir\": \"../cleaned/poyuchen/cleaned_data.csv\",\n",
      "    \"analyzed_data\": \"../cleaned/poyuchen/analyzed_data.csv\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "DATASETS_PATH = \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/\"\n",
    "\n",
    "class TouchtalesDataAnalysis:\n",
    "    def __init__(self, sess_id=0):\n",
    "        # PRESET VARIABLES FOR SAMPLE DATASETS IN ONEDRIVE\n",
    "        # =================================================================\n",
    "        \n",
    "        dataset_paths = [x[0] for x in os.walk(DATASETS_PATH)]\n",
    "        dataset_paths = dataset_paths[1:]\n",
    "\n",
    "        self.datasets = {}\n",
    "        for dataset_path in dataset_paths:\n",
    "            if 'other' in dataset_path:\n",
    "                continue\n",
    "            self.file_paths = [os.path.join(dataset_path, f) for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n",
    "            dataset_name = self.file_paths[0].split('/')[2]\n",
    "            start_info = self.get_start_info(dataset_path)\n",
    "            info = {\n",
    "                'dataset_name': dataset_name,\n",
    "                'date': dataset_name[-4:],\n",
    "                'task_one_ds': self.get_name_containing_substring('TaskOne'),\n",
    "                'task_two_ds': self.get_name_containing_substring('TaskTwo'),\n",
    "                'task_three_ds': self.get_name_containing_substring('TaskThree'),\n",
    "                'task_four_ds': self.get_name_containing_substring('TaskFour'),\n",
    "                'task_five_ds': self.get_name_containing_substring('TaskFive'),\n",
    "                'transcript_ds': self.get_name_containing_substring('.vtt'),\n",
    "                'start_timestamp': start_info[1].strip(),\n",
    "                'video_start': str(int(start_info[0].strip())),\n",
    "                'output_dir': '../cleaned/' + dataset_name + '/cleaned_data.csv',\n",
    "                'analyzed_data': '../cleaned/' + dataset_name + '/analyzed_data.csv'\n",
    "            }\n",
    "            self.datasets[dataset_path] = info\n",
    "\n",
    "        self.datasets_sorted = OrderedDict(sorted(self.datasets.items(), key=lambda x: x[1]['date']))\n",
    "\n",
    "        self.dataset_dict = self.datasets_sorted[list(self.datasets_sorted.keys())[sess_id]]\n",
    "        print(json.dumps(self.dataset_dict, indent=4))\n",
    "\n",
    "        self.dataset_name = self.dataset_dict['dataset_name']\n",
    "        self.task_one_ds = self.dataset_dict['task_one_ds']              # Path to data source for task 1 data\n",
    "        self.task_two_ds = self.dataset_dict['task_two_ds']              # Path to data source for task 2 data\n",
    "        self.task_three_ds = self.dataset_dict['task_three_ds']          # Path to data source for task 3 data\n",
    "        self.task_four_ds = self.dataset_dict['task_four_ds']           # Path to data source for task 4 data\n",
    "        self.task_five_ds = self.dataset_dict['task_five_ds']            # Path to data source for task 5 data\n",
    "        self.transcript_ds = self.dataset_dict['transcript_ds']          # Path to data source for transcript\n",
    "        self.start_timestamp = datetime.strptime(self.dataset_dict['start_timestamp'], '%Y-%m-%d %H:%M:%S.%f')      # Start timestamp for data collection \n",
    "        self.video_start = timedelta(seconds=int(self.dataset_dict['video_start']))             # Start timestamp for video (in s)\n",
    "        self.output_dir = self.dataset_dict['output_dir']\n",
    "        self.analyzed_data = self.dataset_dict['analyzed_data']\n",
    "        self.profiling_report_dir = \"\"\n",
    "\n",
    "    def get_name_containing_substring(self, substring):\n",
    "        for f in self.file_paths:\n",
    "            if substring in f:\n",
    "                return f\n",
    "\n",
    "    def get_start_info(self, data_path):\n",
    "        with open(os.path.join(data_path, 'time.txt')) as f:\n",
    "            lines = f.read().splitlines()\n",
    "            \n",
    "        return lines[0].split(',')\n",
    "\n",
    "tda = TouchtalesDataAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def getEmotion(freqMap):\n",
    "\tres = []\n",
    "\tfor word in freqMap:\n",
    "\t\tisInScale, emotion = inScale(word)\n",
    "\t\tif isInScale:\n",
    "\t\t\tres.append((word,emotion))\n",
    "\tif not res:\n",
    "\t\tres.append((\"no scalable emotion recognized\", None))\n",
    "\n",
    "\treturn res\n",
    "\n",
    "def getSynonyms(word):\n",
    "\tres = []\n",
    "\tfor synset in wn.synsets(word):\n",
    "\t\ts = synset.lemma_names()\n",
    "\t\tif 'sleep_together' not in s:\n",
    "\t\t\tres += s\n",
    "\treturn set(res)\n",
    "\n",
    "def similarEnough(word1, word2, threshold):\n",
    "\tmax_similarity = 0\n",
    "\tfor synset1 in wn.synsets(word1):\n",
    "\t\tfor synset2 in wn.synsets(word2):\n",
    "\t\t\tcurr = wn.path_similarity(synset1, synset2, simulate_root=True)\n",
    "\t\t\tif curr != None and 'sleep_together' not in synset1.lemma_names() and 'sleep_together' not in synset2.lemma_names():\n",
    "\t\t\t\tif curr > max_similarity:\n",
    "\t\t\t\t\tmax_similarity = curr\n",
    "\n",
    "\treturn max_similarity >= threshold\n",
    "\n",
    "def inScale(word, emotions, c_stem=True, c_synonyms=False, c_similarity=False, thres=0.7):\n",
    "\tps = PorterStemmer()\n",
    "\tfor e in emotions:\n",
    "\t\tif (c_stem and ps.stem(e) == ps.stem(word)) or \\\n",
    "\t\t\t(c_synonyms and word in getSynonyms(e)) or \\\n",
    "\t\t\t(c_similarity and similarEnough(word, e, thres)):\n",
    "\t\t\treturn True, e.lower()\n",
    "\treturn False, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"dataset_name\": \"poyuchen\",\n",
      "    \"date\": \"chen\",\n",
      "    \"task_one_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/iag0326/iag0326_TaskOne_752.txt\",\n",
      "    \"task_two_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/iag0326/iag0326_TaskTwo_1679872661016.txt\",\n",
      "    \"task_three_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/iag0326/iag0326_TaskThree_1679874475963.txt\",\n",
      "    \"task_four_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/iag0326/iag0326_TaskFour_1679875274351.txt\",\n",
      "    \"task_five_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/iag0326/iag0326_TaskFive_1679875613385.txt\",\n",
      "    \"transcript_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/iag0326/transcript.vtt\",\n",
      "    \"start_timestamp\": \"2023-03-26 22:53:58.0\",\n",
      "    \"video_start\": \"10\",\n",
      "    \"output_dir\": \"../cleaned/poyuchen/cleaned_data.csv\",\n",
      "    \"analyzed_data\": \"../cleaned/poyuchen/analyzed_data.csv\"\n",
      "}\n",
      "Index(['gsr', 'bpm', 'touch', 'timestamp', 'flag_x', 'gsr_smoothed',\n",
      "       'bpm_smoothed', 'feeltrace', 'flag_y', 'feeltrace_cleaned',\n",
      "       'calibration_1', 'calibration_2', 'comment',\n",
      "       'transcript_timestamp_finish', 'transcript', 'transcript_duration'],\n",
      "      dtype='object')\n",
      "{\n",
      "    \"dataset_name\": \"poyuchen\",\n",
      "    \"date\": \"chen\",\n",
      "    \"task_one_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/asa0413/asa0413_TaskOne_642 (1).txt\",\n",
      "    \"task_two_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/asa0413/asa0413_TaskTwo_1681414846411.txt\",\n",
      "    \"task_three_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/asa0413/asa0413_TaskThree_1681417049702.txt\",\n",
      "    \"task_four_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/asa0413/asa0413_TaskFour_1681422427836.txt\",\n",
      "    \"task_five_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/asa0413/asa0413_TaskFive_1681422507744.txt\",\n",
      "    \"transcript_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/asa0413/transcript.vtt\",\n",
      "    \"start_timestamp\": \"2023-04-13 19:18:51.0\",\n",
      "    \"video_start\": \"9\",\n",
      "    \"output_dir\": \"../cleaned/poyuchen/cleaned_data.csv\",\n",
      "    \"analyzed_data\": \"../cleaned/poyuchen/analyzed_data.csv\"\n",
      "}\n",
      "Index(['gsr', 'bpm', 'touch', 'timestamp', 'flag_x', 'gsr_smoothed',\n",
      "       'bpm_smoothed', 'feeltrace', 'flag_y', 'feeltrace_cleaned',\n",
      "       'calibration_1', 'calibration_2', 'comment',\n",
      "       'transcript_timestamp_finish', 'transcript', 'transcript_duration'],\n",
      "      dtype='object')\n",
      "{\n",
      "    \"dataset_name\": \"poyuchen\",\n",
      "    \"date\": \"chen\",\n",
      "    \"task_one_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/osz0319/nvr0319_TaskOne_2318.txt\",\n",
      "    \"task_two_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/osz0319/nvr0319_TaskTwo_1679266403821.txt\",\n",
      "    \"task_three_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/osz0319/nvr0319_TaskThree_1679270117819.txt\",\n",
      "    \"task_four_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/osz0319/nvr0319_TaskFour_1679272507793.txt\",\n",
      "    \"task_five_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/osz0319/nvr0319_TaskFive_1679272710809.txt\",\n",
      "    \"transcript_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/osz0319/transcript.vtt\",\n",
      "    \"start_timestamp\": \"2023-03-19 22:08:52.0\",\n",
      "    \"video_start\": \"8\",\n",
      "    \"output_dir\": \"../cleaned/poyuchen/cleaned_data.csv\",\n",
      "    \"analyzed_data\": \"../cleaned/poyuchen/analyzed_data.csv\"\n",
      "}\n",
      "Index(['gsr', 'bpm', 'touch', 'timestamp', 'flag_x', 'gsr_smoothed',\n",
      "       'bpm_smoothed', 'feeltrace', 'flag_y', 'feeltrace_cleaned',\n",
      "       'calibration_1', 'calibration_2', 'comment',\n",
      "       'transcript_timestamp_finish', 'transcript', 'transcript_duration'],\n",
      "      dtype='object')\n",
      "{\n",
      "    \"dataset_name\": \"poyuchen\",\n",
      "    \"date\": \"chen\",\n",
      "    \"task_one_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/asa0326/asa0326_TaskOne_920.txt\",\n",
      "    \"task_two_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/asa0326/asa0326_TaskTwo_1679865319283.txt\",\n",
      "    \"task_three_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/asa0326/asa0326_TaskThree_1679868425687.txt\",\n",
      "    \"task_four_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/asa0326/asa0326_TaskFour_1679869508584.txt\",\n",
      "    \"task_five_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/asa0326/asa0326_TaskFive_1679869638143.txt\",\n",
      "    \"transcript_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/asa0326/transcript.vtt\",\n",
      "    \"start_timestamp\": \"2023-03-26 20:41:17.0\",\n",
      "    \"video_start\": \"8\",\n",
      "    \"output_dir\": \"../cleaned/poyuchen/cleaned_data.csv\",\n",
      "    \"analyzed_data\": \"../cleaned/poyuchen/analyzed_data.csv\"\n",
      "}\n",
      "Index(['gsr', 'bpm', 'touch', 'timestamp', 'flag_x', 'gsr_smoothed',\n",
      "       'bpm_smoothed', 'feeltrace', 'flag_y', 'feeltrace_cleaned',\n",
      "       'calibration_1', 'calibration_2', 'comment',\n",
      "       'transcript_timestamp_finish', 'transcript', 'transcript_duration'],\n",
      "      dtype='object')\n",
      "{\n",
      "    \"dataset_name\": \"poyuchen\",\n",
      "    \"date\": \"chen\",\n",
      "    \"task_one_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/asa0411/asa0411b_TaskOne_489.txt\",\n",
      "    \"task_two_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/asa0411/asa0411b_TaskTwo_1681240771910.txt\",\n",
      "    \"task_three_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/asa0411/asa0411b_TaskThree_1681242755019.txt\",\n",
      "    \"task_four_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/asa0411/asa0411b_TaskFour_1681243300686.txt\",\n",
      "    \"task_five_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/asa0411/asa0411b_TaskFive_1681243429237.txt\",\n",
      "    \"transcript_ds\": \"/Users/poyuchen/Desktop/UBC/Engineering-Physics/Fifth-Year/Summer/SPIN/TouchTales-Data-Analysis-main/raw_data/asa0411/transcript.vtt\",\n",
      "    \"start_timestamp\": \"2023-04-11 19:03:53.0\",\n",
      "    \"video_start\": \"18\",\n",
      "    \"output_dir\": \"../cleaned/poyuchen/cleaned_data.csv\",\n",
      "    \"analyzed_data\": \"../cleaned/poyuchen/analyzed_data.csv\"\n",
      "}\n",
      "Index(['gsr', 'bpm', 'touch', 'timestamp', 'flag_x', 'gsr_smoothed',\n",
      "       'bpm_smoothed', 'feeltrace', 'flag_y', 'feeltrace_cleaned',\n",
      "       'calibration_1', 'calibration_2', 'comment',\n",
      "       'transcript_timestamp_finish', 'transcript', 'transcript_duration'],\n",
      "      dtype='object')\n",
      "list index out of range\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "def populate_data_csv(sess_id):\n",
    "    tda = TouchtalesDataAnalysis(sess_id=sess_id)\n",
    "    pipeline = TouchtalesPipeline(tda.dataset_name, tda.task_one_ds, tda.task_two_ds, tda.task_three_ds, tda.task_four_ds, \n",
    "                                tda.task_five_ds, tda.transcript_ds, tda.start_timestamp, tda.video_start, tda.profiling_report_dir, tda.output_dir)\n",
    "    pipeline.clean_data()\n",
    "\n",
    "    _, _, touch, timestamp, flag, gsr, bpm = pipeline.task_one_df.T.to_numpy()\n",
    "    _, timestamp_feeltrace, _, feeltrace, _, _ = pipeline.task_four_df.T.to_numpy()\n",
    "\n",
    "    start = timestamp[0].to_pydatetime()\n",
    "    timestamp_aligned = [(x.to_pydatetime() - start).total_seconds()*1000.0 for x in timestamp.tolist()]\n",
    "\n",
    "    start_feeltrace = timestamp_feeltrace[0].to_pydatetime()\n",
    "    timestamp_aligned_feeltrace = [(x.to_pydatetime() - start_feeltrace).total_seconds()*1000.0 for x in timestamp_feeltrace.tolist()]\n",
    "\n",
    "    flattened_touch = np.array([np.array(x).flatten() for x in touch])\n",
    "    # print(np.array(timestamp_aligned).shape, flattened_touch.shape)\n",
    "    touch_data = np.hstack((np.reshape(timestamp_aligned, (len(timestamp_aligned), 1)), flattened_touch))\n",
    "\n",
    "    gsr_df = pd.DataFrame({'timestamps': timestamp_aligned, 'GSR': gsr})\n",
    "    bpm_df = pd.DataFrame({'timestamps': timestamp_aligned, 'BPM': bpm})\n",
    "    flag_df = pd.DataFrame({'timestamps': timestamp_aligned, 'flag': flag})\n",
    "    touch_df = pd.DataFrame(touch_data, columns=['timestamps']+['T'+str(i) for i in range(1, 101)])\n",
    "    feeltrace_df = pd.DataFrame({'timestamps': timestamp_aligned_feeltrace, 'feeltrace': feeltrace})\n",
    "\n",
    "\n",
    "    # Calibrated Words\n",
    "    transcript = pipeline.transcript_df['transcript'].to_list()\n",
    "    start_list = pipeline.transcript_df['timestamp'].to_list()\n",
    "    duration_list = pipeline.transcript_df['transcript_duration'].to_list()\n",
    "\n",
    "    start_time = start_list[0].to_pydatetime()\n",
    "    timestamps = []\n",
    "    calibrated_words = []\n",
    "    calibrated_values = []\n",
    "    calibrated_labels = [x.lower() for x in pipeline.task_two_df['label'].to_list()]\n",
    "    scales = pipeline.task_two_df['rescaled_df'].to_list()\n",
    "\n",
    "    for i, sentence in enumerate(transcript):\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        length = len(words)\n",
    "        start = start_list[i].to_pydatetime()\n",
    "        duration = duration_list[i]\n",
    "        for j, word in enumerate(words):\n",
    "            isInScale, emotion = inScale(word.lower(), calibrated_labels, \n",
    "                                         c_stem=True, c_synonyms=True, c_similarity=True, thres=0.5)\n",
    "            if isInScale:\n",
    "                pos = float(j+1)/length\n",
    "                word_time = start + pos * duration\n",
    "                timestamps.append((word_time - start_time).total_seconds()*1000.0)\n",
    "                calibrated_words.append(emotion.lower())\n",
    "                # print(calibrated_labels, word, scales[calibrated_labels.index(emotion.lower())])\n",
    "                calibrated_values.append(scales[calibrated_labels.index(emotion.lower())])\n",
    "\n",
    "    calibrated_df = pd.DataFrame({'timestamps': timestamps, 'calibrated_words': calibrated_words, 'calibrated_values': calibrated_values})\n",
    "\n",
    "    gsr_df = gsr_df.drop_duplicates(subset=['timestamps'])\n",
    "    bpm_df = bpm_df.drop_duplicates(subset=['timestamps'])\n",
    "    flag_df = flag_df.drop_duplicates(subset=['timestamps'])\n",
    "    touch_df = touch_df.drop_duplicates(subset=['timestamps'])\n",
    "    feeltrace_df = feeltrace_df.drop_duplicates(subset=['timestamps'])\n",
    "    calibrated_df = calibrated_df.drop_duplicates(subset=['timestamps'])\n",
    "\n",
    "    folder = f'../touchtale/p{sess_id}/'\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    gsr_df.to_csv(folder+'gsr.csv', sep=',', index=False)\n",
    "    bpm_df.to_csv(folder+'bpm.csv', sep=',', index=False)\n",
    "    flag_df.to_csv(folder+'flag.csv', sep=',', index=False)\n",
    "    touch_df.to_csv(folder+'touch.csv', sep=',', index=False)\n",
    "    feeltrace_df.to_csv(folder+'feeltrace.csv', sep=',', index=False)\n",
    "    calibrated_df.to_csv(folder+'calibrated_words.csv', sep=',', index=False)\n",
    "\n",
    "sess_id = 0\n",
    "while True:\n",
    "    try:\n",
    "        populate_data_csv(sess_id)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(sess_id)\n",
    "        break\n",
    "    sess_id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tda \u001b[39m=\u001b[39m TouchtalesDataAnalysis(sess_id\u001b[39m=\u001b[39;49msess_id)\n\u001b[1;32m      2\u001b[0m pipeline \u001b[39m=\u001b[39m TouchtalesPipeline(tda\u001b[39m.\u001b[39mdataset_name, tda\u001b[39m.\u001b[39mtask_one_ds, tda\u001b[39m.\u001b[39mtask_two_ds, tda\u001b[39m.\u001b[39mtask_three_ds, tda\u001b[39m.\u001b[39mtask_four_ds, \n\u001b[1;32m      3\u001b[0m                             tda\u001b[39m.\u001b[39mtask_five_ds, tda\u001b[39m.\u001b[39mtranscript_ds, tda\u001b[39m.\u001b[39mstart_timestamp, tda\u001b[39m.\u001b[39mvideo_start, tda\u001b[39m.\u001b[39mprofiling_report_dir, tda\u001b[39m.\u001b[39moutput_dir)\n\u001b[1;32m      4\u001b[0m pipeline\u001b[39m.\u001b[39mclean_data()\n",
      "Cell \u001b[0;32mIn[4], line 36\u001b[0m, in \u001b[0;36mTouchtalesDataAnalysis.__init__\u001b[0;34m(self, sess_id)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatasets[dataset_path] \u001b[39m=\u001b[39m info\n\u001b[1;32m     34\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatasets_sorted \u001b[39m=\u001b[39m OrderedDict(\u001b[39msorted\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mitems(), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[0;32m---> 36\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatasets_sorted[\u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdatasets_sorted\u001b[39m.\u001b[39;49mkeys())[sess_id]]\n\u001b[1;32m     37\u001b[0m \u001b[39mprint\u001b[39m(json\u001b[39m.\u001b[39mdumps(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_dict, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m))\n\u001b[1;32m     39\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_dict[\u001b[39m'\u001b[39m\u001b[39mdataset_name\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "tda = TouchtalesDataAnalysis(sess_id=sess_id)\n",
    "pipeline = TouchtalesPipeline(tda.dataset_name, tda.task_one_ds, tda.task_two_ds, tda.task_three_ds, tda.task_four_ds, \n",
    "                            tda.task_five_ds, tda.transcript_ds, tda.start_timestamp, tda.video_start, tda.profiling_report_dir, tda.output_dir)\n",
    "pipeline.clean_data()\n",
    "pipeline.task_four_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript_timestamp_finish</th>\n",
       "      <th>transcript</th>\n",
       "      <th>transcript_duration</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-26 22:54:27.510</td>\n",
       "      <td>When you think of your children, what is your...</td>\n",
       "      <td>0 days 00:00:19.280000</td>\n",
       "      <td>2023-03-26 22:54:08.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-26 22:54:30.630</td>\n",
       "      <td>fear that.</td>\n",
       "      <td>0 days 00:00:03.112000</td>\n",
       "      <td>2023-03-26 22:54:27.518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-26 22:54:41.900</td>\n",
       "      <td>I know that there are going to be moments whe...</td>\n",
       "      <td>0 days 00:00:04.160000</td>\n",
       "      <td>2023-03-26 22:54:37.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-26 22:54:46.000</td>\n",
       "      <td>be distant from me, and I know that they're g...</td>\n",
       "      <td>0 days 00:00:04.098000</td>\n",
       "      <td>2023-03-26 22:54:41.902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-26 22:54:50.160</td>\n",
       "      <td>from me. But I think my biggest fear is that ...</td>\n",
       "      <td>0 days 00:00:04.160000</td>\n",
       "      <td>2023-03-26 22:54:46.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  transcript_timestamp_finish  \\\n",
       "0     2023-03-26 22:54:27.510   \n",
       "1     2023-03-26 22:54:30.630   \n",
       "2     2023-03-26 22:54:41.900   \n",
       "3     2023-03-26 22:54:46.000   \n",
       "4     2023-03-26 22:54:50.160   \n",
       "\n",
       "                                          transcript    transcript_duration  \\\n",
       "0   When you think of your children, what is your... 0 days 00:00:19.280000   \n",
       "1                                         fear that. 0 days 00:00:03.112000   \n",
       "2   I know that there are going to be moments whe... 0 days 00:00:04.160000   \n",
       "3   be distant from me, and I know that they're g... 0 days 00:00:04.098000   \n",
       "4   from me. But I think my biggest fear is that ... 0 days 00:00:04.160000   \n",
       "\n",
       "                timestamp  \n",
       "0 2023-03-26 22:54:08.230  \n",
       "1 2023-03-26 22:54:27.518  \n",
       "2 2023-03-26 22:54:37.740  \n",
       "3 2023-03-26 22:54:41.902  \n",
       "4 2023-03-26 22:54:46.000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.transcript_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>y</th>\n",
       "      <th>rescaled_df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>385.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fear</td>\n",
       "      <td>30.237518</td>\n",
       "      <td>414.045982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Shame</td>\n",
       "      <td>55.237518</td>\n",
       "      <td>423.095982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Disgust</td>\n",
       "      <td>113.237518</td>\n",
       "      <td>444.091982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anger</td>\n",
       "      <td>139.237518</td>\n",
       "      <td>453.503982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Embarassment</td>\n",
       "      <td>164.237518</td>\n",
       "      <td>462.553982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sadness</td>\n",
       "      <td>186.237518</td>\n",
       "      <td>470.517982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Surprise</td>\n",
       "      <td>226.237518</td>\n",
       "      <td>484.997982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Envy</td>\n",
       "      <td>249.237518</td>\n",
       "      <td>493.323982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sympathy</td>\n",
       "      <td>338.237518</td>\n",
       "      <td>525.541982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Satisfaction</td>\n",
       "      <td>361.237518</td>\n",
       "      <td>533.867982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pride</td>\n",
       "      <td>378.237518</td>\n",
       "      <td>540.021982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Gratitude</td>\n",
       "      <td>394.237518</td>\n",
       "      <td>545.813982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Happiness</td>\n",
       "      <td>411.237518</td>\n",
       "      <td>551.967982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Love</td>\n",
       "      <td>433.237518</td>\n",
       "      <td>559.931982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>peace</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>566.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label           y  rescaled_df\n",
       "15       anxiety  -50.000000   385.000000\n",
       "1           Fear   30.237518   414.045982\n",
       "13         Shame   55.237518   423.095982\n",
       "4        Disgust  113.237518   444.091982\n",
       "0          Anger  139.237518   453.503982\n",
       "6   Embarassment  164.237518   462.553982\n",
       "3        Sadness  186.237518   470.517982\n",
       "5       Surprise  226.237518   484.997982\n",
       "7           Envy  249.237518   493.323982\n",
       "10      Sympathy  338.237518   525.541982\n",
       "12  Satisfaction  361.237518   533.867982\n",
       "8          Pride  378.237518   540.021982\n",
       "11     Gratitude  394.237518   545.813982\n",
       "2      Happiness  411.237518   551.967982\n",
       "9           Love  433.237518   559.931982\n",
       "14         peace  450.000000   566.000000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.task_two_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beloved',\n",
       " 'dear',\n",
       " 'dearest',\n",
       " 'enjoy',\n",
       " 'erotic_love',\n",
       " 'honey',\n",
       " 'love',\n",
       " 'love_life',\n",
       " 'lovemaking',\n",
       " 'making_love',\n",
       " 'passion',\n",
       " 'sexual_love'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getSynonyms('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarEnough('love', 'know', 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rescaled_df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>385.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>414.045982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>423.095982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>444.091982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>453.503982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>462.553982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>470.517982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>484.997982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>493.323982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>525.541982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>533.867982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>540.021982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>545.813982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>551.967982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>559.931982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>566.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rescaled_df\n",
       "15   385.000000\n",
       "1    414.045982\n",
       "13   423.095982\n",
       "4    444.091982\n",
       "0    453.503982\n",
       "6    462.553982\n",
       "3    470.517982\n",
       "5    484.997982\n",
       "7    493.323982\n",
       "10   525.541982\n",
       "12   533.867982\n",
       "8    540.021982\n",
       "11   545.813982\n",
       "2    551.967982\n",
       "9    559.931982\n",
       "14   566.000000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.task_two_df.loc[:, pipeline.task_two_df.columns.str.contains('rescaled_df')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
